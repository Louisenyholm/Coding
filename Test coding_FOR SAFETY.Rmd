---
title: "Test coding"
author: "Louise"
date: "23/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading packages
pacman::p_load(tidyverse, brms, bayesplot, rethinking, data.table)
```


Fancy model example
```{r}
# globe_qa_brms <-
#   brm(data = list(w = 24), 
#       family = binomial(link = "identity"),
#       w | trials(36) ~ 1,
#       prior(beta(1, 1), class = Intercept),
#       iter = 4000, warmup = 1000,
#       control = list(adapt_delta = .9),
#       seed = 4)
```


Faking some data (the easy and not representational way)
```{r}
d_fake <- tibble(
  ID = c(1:40),
  incorrect = c(rep(0, 30), rep(1, 10)),
  RT = c(rnorm(20, 800, 100), rnorm(20, 900, 150)),
  consistency = c(rep(0:1, 20)),
  condition = c(rep(0:1, 20)),
  empathy = rnorm(40, 77.5, 23)
)

d_fake$ID <- as.factor(d_fake$ID)
d_fake$incorrect <- as.factor(d_fake$incorrect)
d_fake$consistency <- as.factor(d_fake$consistency)
d_fake$condition <- as.factor(d_fake$condition)
d_fake$empathy <- as.integer(d_fake$empathy)
```


Reading example data
```{r}
# NOT NEEDED
#d3 <- read_csv("data_times.csv") (only data times)

# Loading data
d1 <- read_csv("data.csv")          # All participants info + emp
# p <- read.table("DPT_FEMALE.2020-04-29-1628.data.6fa8424a-a872-459f-b3a8-d60c12f0fe2b.txt", quote="\"", comment.char="")   # Single participant experimental trials

# Preprocessing

# Reversing empathy item 1, 2, 17, 29
d1[ ,c(10,11,26,38)] = 5 - d1[ ,c(10,11,26,38)]

# Calculating empathy (Cognitive Empathy, Affective Empathy, Total Empathy Score)
d1$cogemp <- rowSums(d1[, c(10, 12:15, 24:25, 27:31, 33:37, 39:40)])
d1$affemp <- rowSums(d1[, c(11, 16:23, 26, 32, 38)])
d1$totalemp <- d1$cogemp + d1$affemp

# Gender into M and F
d1$gender <- ifelse(d1$`sex:1` == 1, "M", "F")

# Nationality (native_language) and siblings
d1$native <- 0
d1$siblings <- NA

for(i in 1:nrow(d1)){
  d1$native[i] <- ifelse(d1[i,4] == 1, "danish",
                     ifelse(d1[i,4] == 2, "english",
                            ifelse(d1[i,4] == 3, "german",
                                   d1[i,5])))
  d1$siblings[i] <- ifelse(d1[i, 6] == 5, d1[i, 7], d1[i, 6])
  }

# Renaming age column
names(d1)[2] <- "age"

# Matching the names in order to merge (participant remvoving s.)
d1$participant <- gsub("s.", "", d1$participant)


# New dataframe without all the empathy columns
emp <- d1[, c("participant", "gender", "age", "native", "siblings", "cogemp", "affemp", "totalemp")]



# Filename for merging
#setwd("C:/Users/louis/OneDrive - Aarhus universitet/AU Onedrive - RIGTIG/- 4. Semester/Social and Cultural Dynamics in Cognition/Exam related/Coding")

# read file path
all_paths <-
  list.files(path = "C:/Users/louis/OneDrive - Aarhus universitet/AU Onedrive - RIGTIG/- 4. Semester/Social and Cultural Dynamics in Cognition/Exam related/Coding",
             pattern = "*.txt",
             full.names = TRUE)
 
# read file content
all_content <-
  all_paths %>%
  lapply(read.table,
         quote="\"",
         comment.char="")
 
# read file name
all_filenames <- all_paths %>%
  basename() %>%
  as.list()
 
# combine file content list and file name list
all_lists <- mapply(c, all_content, all_filenames, SIMPLIFY = FALSE)
 
# unlist all lists and change column name
all_result <- rbindlist(all_lists, fill = T)

# change column name
names(all_result) <- c("block", "stimuli", "condition", "direction", "self", "other", "consistency", "cue", "correctresponse", "response", "accuracy", "RT", "participant")


# Function for keeping the last part
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}
 # Using the function to keep 40 last characters
all_result$participant <- substrRight(all_result$participant, 40)

# Button and correctbutton
all_result$correctresponse <- ifelse(all_result$correctresponse == 1, "yes", "no")
all_result$response <- ifelse(all_result$response == 1, "yes", "no")
all_result$response <- ifelse(all_result$accuracy == 3, "late", all_result$response)

# Accuracy
all_result$accuracy <- ifelse(all_result$accuracy == 1, "correct", ifelse(all_result$accuracy ==2, "incorrect", "late"))


### Merging by participant
d <- merge(emp, all_result, by = "participant")

# Creating indices for each participant
d$ID <- d %>%
  group_indices(participant)
```


Reaction Time - Defining models
```{r}
# Baseline model
RT_f0 <- bf(RT ~ 1)
#Maybe a model with consistency alone?
# Condition + consistency model
RT_f1 <- bf(RT ~ 1 + condition + consistency)
# Interaction model
RT_f2 <- bf(RT ~ 0 + condition + condition : consistency)

# Multilevel baseline model
RT_f0_ <- bf(RT ~ 1 + (1|ID))
#Maybe a model with consistency alone?
# Multilevel consistency model
RT_f1_ <- bf(RT ~ 1 + condition + consistency + (1 + condition + consistency|ID))
# Multilevel interaction model
RT_f2_ <- bf(RT ~ 0 + condition + condition : consistency + (0 + condition + condition : consistency | ID))
```

RT - F0 - Setting priors
```{r}
# RT_F0
# Which priors to define?
get_prior(RT_f0, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f0 <- c(
  prior(normal(6.7, 0.3), class = Intercept),
  prior(normal(0, 0.1), class = sigma)
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?

# Testing the priors
l <- rlnorm(10000, 6.7 , 0.3) # sampling from the lognormal

l <- rlnorm(10000,
            6.7 +
              rnorm(10000, 0, .1), # maybe this is like adding the sigma
            0.3)

dens(l)

# Testing the prior in the model
RT_m0_prior <- brm(
  RT_f0,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f0,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m0_prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m0_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F0 - Fitting the model
```{r}
# Fitting the model
RT_m0 <- brm(
  RT_f0,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f0,
  sample_prior = T
)

# Quality check of the model
summary(RT_m0) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m0, pars = c("b_Intercept", "sigma", "ndt")) + theme_classic()
mcmc_rank_overlay(RT_m0, pars = c("b_Intercept", "sigma", "ndt")) + theme_classic()

# Standard pp_check
pp_check(RT_m0, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m0,"Intercept > 0"))
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# It does not plot the prior for Intercept.
```

RT - F1 - Setting priors
```{r}
# RT_F1
# Which priors to define?
get_prior(RT_f1, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f1 <- c(
  prior(normal(6.7, 0.3), class = Intercept), # = mean RT
  prior(normal(0, 0.1), class = b, coef = condition1),
  prior(normal(0, 0.1), class = b, coef = consistency1),
  prior(normal(0, 0.1), class = sigma)
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?

# Testing the priors
l <- rlnorm(10000,
            6.7 +
              (runif(10000, 0, 1) * # actual values of predictor (min, max)
              rnorm(10000, 0, 0.1)) +
              (runif(10000, 0, 1) *
              rnorm(10000, 0, 0.1)), # values of beta related to that predictor
            0.3)

l <- rlnorm(10000,
            6.7 +
              (runif(10000, 0, 1) *   # actual values of predictor (min, max)
              rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
              (runif(10000, 0, 1) *   # actual values of predictor (min, max)
              rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
              rnorm(10000, 0, 0.1),   # sigma
            0.3)
dens(l)

# Testing the prior in the model
RT_m1_prior <- brm(
  RT_f1,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f1,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m1_prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m1_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F1 - Fitting the model
```{r}
# Fitting the model
RT_m1 <- brm(
  RT_f1,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f1,
  sample_prior = T
)

# Quality check of the model
summary(RT_m1) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m1, pars = c("b_Intercept", "b_condition1", "b_consistency1", "sigma", "ndt")) + theme_classic()
mcmc_rank_overlay(RT_m1, pars = c("b_Intercept", "b_condition1", "b_consistency1", "sigma", "ndt")) + theme_classic()

# Standard pp_check
pp_check(RT_m1, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m1,"Intercept > 0"))
plot(hypothesis(RT_m1,"condition1 > 0")) 
plot(hypothesis(RT_m1,"consistency1 > 0")) 
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# NDT???
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learned)?
# Not actual hypothesis testing here...
```

RT - F2 - Setting priors
```{r}
# RT_F2
# Which priors to define?
get_prior(RT_f2, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f2 <- c(
  prior(normal(6.7, 0.3), class = b, coef = condition0), # mean RT in arrow
  prior(normal(6.7, 0.3), class = b, coef = condition1), # mean RT in avatar
  prior(normal(0, 0.1), class = b, coef = condition0:consistency1), # mean RT effect in arrow going consistent to inconsistent
  prior(normal(0, 0.1), class = b, coef = condition1:consistency1), # mean RT effect in avatar going consistent to inconsistent
  prior(normal(0, 0.1), class = sigma)
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?

# # Testing the priors
# l <- rlnorm(10000,
#             6.7 +
#               (runif(10000, 0, 1) * # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) +
#               (runif(10000, 0, 1) *
#               rnorm(10000, 0, 0.1)), # values of beta related to that predictor
#             0.3)
# 
# l <- rlnorm(10000,
#             6.7 +
#               (runif(10000, 0, 1) *   # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
#               (runif(10000, 0, 1) *   # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
#               rnorm(10000, 0, 0.1),   # sigma
#             0.3)
# dens(l)

# Testing the prior in the model
RT_m2_prior <- brm(
  RT_f2,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f2,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m2_prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m2_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F2 - Fitting the model
```{r}
# Fitting the model
RT_m2 <- brm(
  RT_f2,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f2,
  sample_prior = T
)

# Quality check of the model
summary(RT_m2) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m2, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sigma", "ndt")) + theme_classic()
mcmc_rank_overlay(RT_m2, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sigma", "ndt")) + theme_classic()

# Standard pp_check
pp_check(RT_m2, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m2,"condition0 > 0")) # has learned
plot(hypothesis(RT_m2,"condition1 > 0")) # has learned
plot(hypothesis(RT_m2,"condition0:consistency1 > 0")) # interaction not made in fake data, so no difference from prior
plot(hypothesis(RT_m2,"condition1:consistency1 > 0"))
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# NDT???
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learned)?
# Not actual hypothesis testing here...
```

RT - F0_ (Multilevel) - Setting priors
```{r}
# RT_F0_ (Multilevel)
# Which priors to define?
get_prior(RT_f0_, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f0_ <- c(
  prior(normal(6.7, 0.3), class = Intercept),
  prior(normal(0, 0.1), class = sigma),
  prior(normal(0, 0.1), class = sd)
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?

# Testing the priors
l <- rlnorm(10000, 6.7 , 0.3) # sampling from the lognormal

l <- rlnorm(10000,
            6.7 +
              rnorm(10000, 0, .1) + # maybe this is like adding the sigma
              rnorm(10000, 0, .1), # and sd
            0.3)
dens(l)

# Testing the prior in the model
RT_m0__prior <- brm(
  RT_f0_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f0_,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m0__prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m0__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F0_ (Multilevel) - Fitting the model
```{r}
# Fitting the model
RT_m0_ <- brm(
  RT_f0_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f0_,
  sample_prior = T
)

# Quality check of the model
summary(RT_m0_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m0_, pars = c("b_Intercept", "sigma", "ndt", "sd_ID__Intercept")) + theme_classic()
mcmc_rank_overlay(RT_m0_, pars = c("b_Intercept", "sigma", "ndt", "sd_ID__Intercept")) + theme_classic()

# Standard pp_check
pp_check(RT_m0_, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m0,"Intercept > 0"))
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# and NDT?
# and SD?
# It does not plot the prior for Intercept.
```

RT - F1_ (Multilevel) - Setting priors
```{r}
# RT_F1_ (Multilevel)
# Which priors to define?
get_prior(RT_f1_, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f1_ <- c(
  prior(normal(6.7, 0.3), class = Intercept), # = mean RT
  prior(normal(0, 0.1), class = b, coef = condition1),
  prior(normal(0, 0.1), class = b, coef = consistency1),
  prior(normal(0, 0.1), class = sigma),
  prior(normal(0, 0.1), class = sd), # Variation per ID across both predictors
  prior(lkj(5), class = cor) # should this be defined in log as well
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?


# Testing the priors
l <- rlnorm(10000,
            6.7 +
              (runif(10000, 0, 1) *   # actual values of predictor (min, max)
              rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
              (runif(10000, 0, 1) *   # actual values of predictor (min, max)
              rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
              rnorm(10000, 0, 0.1),   # sigma
              rnorm(10000, 0, 0.1),   # sd
            0.3)
dens(l)

# Testing the prior in the model
RT_m1__prior <- brm(
  RT_f1_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f1_,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m1__prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m1__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F1_ (Multilevel) - Fitting the model
```{r}
# Fitting the model
RT_m1_ <- brm(
  RT_f1_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f1_,
  sample_prior = T
)

# Quality check of the model
summary(RT_m1_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m1_, pars = c("b_Intercept", "b_condition1", "b_consistency1", "sigma", "ndt", "sd_ID__Intercept", "sd_ID__condition1", "sd_ID__consistency1", "cor_ID__Intercept__condition1", "cor_ID__Intercept__consistency1", "cor_ID__condition1__consistency1")) + theme_classic()
mcmc_rank_overlay(RT_m1_, pars = c("b_Intercept", "b_condition1", "b_consistency1", "sigma", "ndt", "sd_ID__Intercept", "sd_ID__condition1", "sd_ID__consistency1", "cor_ID__Intercept__condition1", "cor_ID__Intercept__consistency1", "cor_ID__condition1__consistency1")) + theme_classic()

# Standard pp_check
pp_check(RT_m1_, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m1_,"Intercept > 0"))
plot(hypothesis(RT_m1_,"condition1 > 0")) 
plot(hypothesis(RT_m1_,"consistency1 > 0")) 
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# NDT???
# SD??
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learned)?
# Not actual hypothesis testing here...
```

RT - F2_ (Multilevel) - Setting priors
```{r}
# RT_F2_ (Multilevel)
# Which priors to define?
get_prior(RT_f2_, d_fake, family = shifted_lognormal())

# Setting priors
RT_prior_f2_ <- c(
  prior(normal(6.7, 0.3), class = b, coef = condition0), # mean RT in arrow
  prior(normal(6.7, 0.3), class = b, coef = condition1), # mean RT in avatar
  prior(normal(0, 0.1), class = b, coef = condition0:consistency1), # mean RT effect in arrow going consistent to inconsistent
  prior(normal(0, 0.1), class = b, coef = condition1:consistency1), # mean RT effect in avatar going consistent to inconsistent
  prior(normal(0, 0.1), class = sigma),
  prior(normal(0, 0.1), class = sd), # Variation per ID across both predictors
  prior(lkj(5), class = cor) # should this be defined in log as well
  # prior(normal(nn, nn), class = ndt)
)
# If 6.7 = 812 ms (exp(6.7) = 812)
# Does it mirror our expectations/the overall mean of the data?

# # Testing the priors
# l <- rlnorm(10000,
#             6.7 +
#               (runif(10000, 0, 1) * # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) +
#               (runif(10000, 0, 1) *
#               rnorm(10000, 0, 0.1)), # values of beta related to that predictor
#             0.3)
# 
# l <- rlnorm(10000,
#             6.7 +
#               (runif(10000, 0, 1) *   # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
#               (runif(10000, 0, 1) *   # actual values of predictor (min, max)
#               rnorm(10000, 0, 0.1)) + # values of beta related to that predictor
#               rnorm(10000, 0, 0.1),   # sigma
#             0.3)
# dens(l)

# Testing the prior in the model
RT_m2__prior <- brm(
  RT_f2_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f2_,
  sample_prior = "only"
)

# Standard pp_check
pp_check(RT_m2__prior, nsamples = 100)

# Another pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
# y_pred <- posterior_linpred(RT_m2__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
# dens(exp(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

RT - F2_ (Multilevel) - Fitting the model
```{r}
# Fitting the model
RT_m2_ <- brm(
  RT_f2_,
  d_fake,
  family = shifted_lognormal(),
  prior = RT_prior_f2_,
  sample_prior = T
)

# Quality check of the model
summary(RT_m2_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(RT_m2_, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sigma", "ndt", "sd_ID__condition0", "sd_ID__condition1", "sd_ID__condition0:consistency1", "sd_ID__condition1:consistency1", "cor_ID__condition0__condition1", "cor_ID__condition0__condition0:consistency1", "cor_ID__condition1__condition0:consistency1", "cor_ID__condition0__condition1:consistency1", "cor_ID__condition1__condition1:consistency1", "cor_ID__condition0:consistency1__condition1:consistency1")) + theme_classic()
mcmc_rank_overlay(RT_m2_, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sigma", "ndt", "sd_ID__condition0", "sd_ID__condition1", "sd_ID__condition0:consistency1", "sd_ID__condition1:consistency1", "cor_ID__condition0__condition1", "cor_ID__condition0__condition0:consistency1", "cor_ID__condition1__condition0:consistency1", "cor_ID__condition0__condition1:consistency1", "cor_ID__condition1__condition1:consistency1", "cor_ID__condition0:consistency1__condition1:consistency1")) + theme_classic()

# Standard pp_check
pp_check(RT_m2_, nsamples = 100)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(RT_m2_,"condition0 > 0")) # has learned
plot(hypothesis(RT_m2_,"condition1 > 0")) # has learned
plot(hypothesis(RT_m2_,"condition0:consistency1 > 0")) # interaction not made in fake data, so no difference from prior
plot(hypothesis(RT_m2_,"condition1:consistency1 > 0"))
# plot(hypothesis(RT_m0, "sigma > 0")) # Something like this for sigma?
# NDT???
# SD???
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learned)?
# Not actual hypothesis testing here...
```

Comparing RT models for hypothesis 1
```{r}
# Adding criterion
RT_m0 <- add_criterion(m0, criterion = c("bayes_R2", "loo"), reloo = T)
RT_m1 <- add_criterion(m1, criterion = c("bayes_R2", "loo"), reloo = T)
RT_m2 <- add_criterion(m2, criterion = c("bayes_R2", "loo"), reloo = T)
RT_m0_ <- add_criterion(m0_, criterion = c("bayes_R2", "loo"), reloo = T)
RT_m1_ <- add_criterion(m1_, criterion = c("bayes_R2", "loo"), reloo = T)
RT_m2_ <- add_criterion(m2_, criterion = c("bayes_R2", "loo"), reloo = T)

# Comparing out of sample error
loo_compare(RT_m0, RT_m1, RT_m2, RT_m0_, RT_m1_, RT_m2_)

# Stacking weights
loo_model_weights(RT_m0, RT_m1, RT_m2, RT_m0_, RT_m1_, RT_m2_)

```



RF: Chimp-video part 2:
We want to see the propensity of participants to be affected by altercentric intrusion/consistency effect (across conditions) (that is, their propensity to make errors on the DPT in consistent vs. inconsistent and arrow vs. avatar condition).
We hope to see that that propensity is modulated by the level of consistency and condition (whether there is an arrow or avatar in the stimulus). It only makes sense as a social process, if it is higher when there is an avatar compared to an arrow in the middle (however, other processes (directional cueing) can account for the effect, if it is found in the arrow condition as well).

Outcome: 1 vs. 0 (has the participant made an incorrect vs correct response)
We are trying to reconstruct the probability of making an error (/choosing prosocial).


Error - Defining models
```{r}
# Baseline model
incorrect_f0 <- bf(incorrect ~ 1) # Fixed level of errors across all conditions
#Maybe a model with consistency alone?
# Condition + consistency model
incorrect_f1 <- bf(incorrect ~ 1 + condition + consistency) # Expecting that the level of errors will be affected by the level of condition/consistency.
# Interaction model
incorrect_f2 <- bf(incorrect ~ 0 + condition + condition : consistency)

# Multilevel baseline model
incorrect_f0_ <- bf(incorrect ~ 1 + (1|ID)) # Partially pooling per participant (more far away are pooled more than closer to the overall mean + more data than others will pull the participant less - not relevant to us though)
#Maybe a model with consistency alone?
# Multilevel consistency model
incorrect_f1_ <- bf(incorrect ~ 1 + condition + consistency + (1 + condition + consistency|ID))
# Multilevel interaction model
incorrect_f2_ <- bf(incorrect ~ 0 + condition + condition : consistency + (0 + condition + condition : consistency | ID))
```

Error - F0 - Setting priors
```{r}
# F0
# Which priors to define?
get_prior(incorrect_f0, d_fake, family = bernoulli())

# Setting priors
prior_in_f0 <- c(
  prior(normal(0, 1.5), class = Intercept) # = mean prob of error (what do we assume)
)
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.
# If -0.8 = 0.31 = 31 % prob of incorrect (but how much do we know at this point?)
# Does it mirror our expectations?

# Testing the priors
p <- rnorm(10000, -0.7, 1)
dens(p) # on the log-odds scale/space (insensible)
dens(inv_logit(p)) # on the outcome/probability scale/space (0 = always correct, 1 = always incorrect)

# Testing the prior in the model
m0_prior <- brm(
  incorrect_f0,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f0,
  sample_prior = "only"
)

# Standard pp_check
# pp_check(m0_prior, nsamples = 100) Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m0_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

Error - F0 - Fitting the model
```{r}
# Fitting the model
m0 <- brm(
  incorrect_f0,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f0,
  sample_prior = T
)

# Quality check of the model
summary(m0) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m0, pars="b_Intercept") + theme_classic()
mcmc_rank_overlay(m0, pars="b_Intercept") + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m0)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (= 0). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m0,"Intercept < 0")) 
# Log-odds scale (0 = chance level). We think, the chance is higher for correct (0) than for incorrect (1), thus below chance level probability of incorrect.
# It does not plot the prior for Intercept.
```


Error - F1 - Setting priors
```{r}
# F1
# Which priors to define?
get_prior(incorrect_f1, d_fake, family = bernoulli())

# Setting priors
prior_in_f1 <- c(
  prior(normal(0, 1.5), class = Intercept), # = mean prob of error (same as before)
  prior(normal(0, 0.3), class = b, coef = condition1),
  prior(normal(0, 0.3), class = b, coef = consistency1)
)
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.
# If -0.8 = 0.31 = 31 % prob of incorrect (but how much do we know at this point?)
# Does it mirror our expectations?

# Testing the priors
p <- inv_logit(rnorm(10000, 0, 1.5) +
                 rnorm(10000, 0, 0.3) +
                 rnorm(10000, 0, 0.3))
dens(p) # on the outcome/probability scale/space (0% incorrect = always correct, 100% incorrect = always incorrect)
# Looks like what we expected to see - something which just removes a bit of the probability of the extremes

# Testing the prior in the model
m1_prior <- brm(
  incorrect_f1,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f1,
  sample_prior = "only"
)

# Standard pp_check
# pp_check(m1_prior, nsamples = 100) Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m1_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

Error - F1 - Fitting the model
```{r}
# Fitting the model
m1 <- brm(
  incorrect_f1,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f1,
  sample_prior = T
)

# Quality check of the model
summary(m1) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m1, pars = "b_Intercept", c("b_condition1", "b_consistency1")) + theme_classic() # we can ignore the lp (log-probability)
mcmc_rank_overlay(m1, pars = "b_Intercept", c("b_condition1", "b_consistency1")) + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m1)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (25% probability of answering incorrect). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m1,"condition1 > 0")) 
plot(hypothesis(m1,"consistency1 > 0")) 
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learnt)?
# He does not mention anything about actual hypothesis testing here...
```

Error - F2 - Setting priors
```{r}
# F2
# Which priors to define?
get_prior(incorrect_f2, d_fake, family = bernoulli())

# Setting priors
prior_in_f2 <- c(
  prior(normal(0, 1.5), class = b, coef = condition0), # mean prob of error in arrow
  prior(normal(0, 1.5), class = b, coef = condition1), # mean prob of error in avatar
  prior(normal(0, 0.3), class = b, coef = condition0:consistency1), # mean prob difference/effect in arrow going consistent to inconsistent
  prior(normal(0, 0.3), class = b, coef = condition1:consistency1) # mean prob difference/effect in avatar going consisten to inconsistent
)
# Setting all the betas to be like the original intercept (in the 1+ models above 34:10 in RF's video). (the condition betas which represent the intercept), and the : betas reflect an effect, so we set them as the effect from before.
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.

# Testing the priors (DOES NOT SEEM TO REFLECT THE ABOVE/BELOW!)
p <- inv_logit(rnorm(10000, 0, 1.5) +
                 rnorm(10000, 0, 1.5) +
                 rnorm(10000, 0, 0.3) +
                 rnorm(10000, 0, 0.3))
dens(p)
# on the outcome/probability scale/space (0% incorrect = always correct, 100% incorrect = always incorrect)
# Looks like what we expected to see - something which just removes a bit of the probability of the extremes

# Testing the prior in the model
m2_prior <- brm(
  incorrect_f2,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f2,
  sample_prior = "only"
)

# Standard pp_check
# pp_check(m2_prior, nsamples = 1000) #Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m2_prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
dens(posterior_linpred(m2_prior, transform = T)) # Easy way to do the above two lines
```

Error - F2 - Fitting the model
```{r}
# Fitting the model
m2 <- brm(
  incorrect_f2,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f2,
  sample_prior = T
)

# Quality check of the model
summary(m2) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m2, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1")) + theme_classic() # we can ignore the lp (log-probability)
mcmc_rank_overlay(m2, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1")) + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m2)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (25% probability of answering incorrect). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m2,"condition0 > 0")) # has learned
plot(hypothesis(m2,"condition1 > 0")) # has learned
plot(hypothesis(m2,"condition0:consistency1 > 0")) # has not really gotten more confindent. One could go back to the prior and make sure that you do not too strongly influence the posterior
plot(hypothesis(m2,"condition1:consistency1 > 0"))
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learnt)?
# He does not mention anything about actual hypothesis testing here...
```

Error - F0_ (Multilevel) - Setting priors
```{r}
# F0_ (Multilevel)
# Which priors to define?
get_prior(incorrect_f0_, d_fake, family = bernoulli())

# Setting priors
prior_in_f0_ <- c(
  prior(normal(0, 1.5), class = Intercept), # mean prob of error in arrow
  prior(normal(0, 0.3), class = sd) # SD of intercept by ID (how much we expect the participants to vary)
)
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.

# Testing the priors (DOES NOT SEEM TO REFLECT THE ABOVE/BELOW!)
p <- inv_logit(rnorm(100000, 0, 1.5) +
                 rnorm(100000, 0, 0.3))
dens(p)
# on the outcome/probability scale/space (0% incorrect = always correct, 100% incorrect = always incorrect)
# Looks like what we expected to see - something which just removes a bit of the probability of the extremes

# Testing the prior in the model
m0__prior <- brm(
  incorrect_f0_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f0_,
  sample_prior = "only"
)

# Standard pp_check
# pp_check(m2_prior, nsamples = 1000) #Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m0__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

Error - F0_ (Multilevel) - Fitting the model
```{r}
# Fitting the model
m0_ <- brm(
  incorrect_f0_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f0_,
  sample_prior = T
)

# Quality check of the model
summary(m0_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m0_, pars = c("b_Intercept", "sd_ID__Intercept")) + theme_classic() # we can ignore the lp (log-probability).
# Is it enough to look at the general sd, or per ID?
mcmc_rank_overlay(m0_, pars = c("b_Intercept", "sd_ID__Intercept")) + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m0_)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (25% probability of answering incorrect). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m0_,"Intercept > 0"))
# plot(hypothesis(m0_,"sd > 0")) # we should also check whether the sd has learned (he does not, but tells us to)
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learnt)?
# He does not mention anything about actual hypothesis testing here...
```

Error - F1_ (Multilevel) - Setting priors
```{r}
# F1_ (Multilevel)
# Which priors to define?
get_prior(incorrect_f1_, d_fake, family = bernoulli())

# Setting priors
prior_in_f1_ <- c(
  prior(normal(0, 1.5), class = Intercept), # = mean prob of error (same as before)
  prior(normal(0, 0.3), class = b, coef = condition1),
  prior(normal(0, 0.3), class = b, coef = consistency1),
  prior(normal(0, 0.3), class = sd), # Variation per ID across both predictors
  prior(lkj(5), class = cor) # Value for correlations, discounting values close to -1 and 1.
)
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.
# If -0.8 = 0.31 = 31 % prob of incorrect (but how much do we know at this point?)
# Does it mirror our expectations?

# Testing the prior in the model
m1__prior <- brm(
  incorrect_f1_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f1_,
  sample_prior = "only"
)

# Standard pp_check
pp_check(m1__prior, nsamples = 100) #Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m1__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
```

Error - F1_ - Fitting the model
```{r}
# Fitting the model
m1_ <- brm(
  incorrect_f1_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f1_,
  sample_prior = T
)

# Quality check of the model
summary(m1_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m1_, pars = "b_Intercept", c("b_condition1", "b_consistency1", "sd_ID__Intercept", "sd_ID__condition1", "sd_ID__consistency1", "cor_ID__Intercept__condition1", "cor_ID__Intercept__consistency1", "cor_ID__condition1__consistency1")) + theme_classic() # we can ignore the lp (log-probability)
mcmc_rank_overlay(m1_, pars = "b_Intercept", c("b_condition1", "b_consistency1", "sd_ID__Intercept")) + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m1_)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (25% probability of answering incorrect). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m1_,"Intercept > 0")) 
plot(hypothesis(m1_,"condition1 > 0"))
plot(hypothesis(m1_,"consistency1 > 0")) 
# plot(hypothesis(m1_,"Intercept > 0", class = sd)) # we should also check whether the sd has learned (he does not, but tells us to)
# Has not learned much. Maybe make priors wider...
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learnt)?
# He does not mention anything about actual hypothesis testing here...
```

Error - F2_ (Multilevel) - Setting priors
```{r}
# F2_ (Multilevel)
# Which priors to define?
get_prior(incorrect_f2_, d_fake, family = bernoulli())

# Setting priors
prior_in_f2_ <- c(
  prior(normal(0, 1.5), class = b, coef = condition0), # mean prob of error in arrow
  prior(normal(0, 1.5), class = b, coef = condition1), # mean prob of error in avatar
  prior(normal(0, 0.3), class = b, coef = condition0:consistency1), # mean prob difference/effect in arrow going consistent to inconsistent
  prior(normal(0, 0.3), class = b, coef = condition1:consistency1), # mean prob difference/effect in avatar going consisten to inconsistent
  prior(normal(0, 0.3), class = sd),
  prior(lkj(5), class = cor)
)
# Setting all the betas to be like the original intercept (in the 1+ models above 34:10 in RF's video). (the condition betas which represent the intercept), and the : betas reflect an effect, so we set them as the effect from before.
# If 0 = centering at chance (inv_logit of 0 = 0.5), making it "random" whether it is correct or incorrect. We do not know much yet - going from 0 to 1, discounting the extreme values a bit.

# Testing the prior in the model
m2__prior <- brm(
  incorrect_f2_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f2_,
  sample_prior = "only"
)

# Standard pp_check
# pp_check(m2__prior, nsamples = 1000) #Bypass this

# Better pp_check (when starting to add more parameters, it will be difficult to run ad hoc check with the standard pp_check)
y_pred <- posterior_linpred(m2__prior) # Generating predictions from the models, not 0s and 1s, but expected rates (thus we want linear prediction - in log-odds space)
dens(inv_logit(y_pred)) # Into probability space (almost uniform distribution with discounting of values of 0 and 1)
dens(posterior_linpred(m2__prior, transform = T)) # Easy way to do the above two lines
```

Error - F2_ (Multilevel) - Fitting the model
```{r}
# Fitting the model
m2_ <- brm(
  incorrect_f2_,
  d_fake,
  family = bernoulli(),
  prior = prior_in_f2_,
  sample_prior = T
)

# Quality check of the model
summary(m2_) # Checking for warnings of divergences and Rhat, ESS

# Trace plots and trace rank plots (Only interested in intercept, the only parameter we have)
color_scheme_set("viridis")
mcmc_trace(m2_, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sd_ID__condition0", "sd_ID__condition1", "sd_ID__condition0:consistency1", "sd_ID__condition1:consistency1", "cor_ID__condition0__condition1", "cor_ID__condition0__condition0:consistency1", "cor_ID__condition1__condition0:consistency1", "cor_ID__condition0__condition1:consistency1",  "cor_ID__condition1__condition1:consistency1", "cor_ID__condition0:consistency1__condition1:consistency1")) + theme_classic() # we can ignore the lp (log-probability)
mcmc_rank_overlay(m2_, pars = c("b_condition0", "b_condition1", "b_condition0:consistency1", "b_condition1:consistency1", "sd_ID__condition0", "sd_ID__condition1", "sd_ID__condition0:consistency1", "sd_ID__condition1:consistency1", "cor_ID__condition0__condition1", "cor_ID__condition0__condition0:consistency1", "cor_ID__condition1__condition0:consistency1", "cor_ID__condition0__condition1:consistency1",  "cor_ID__condition1__condition1:consistency1", "cor_ID__condition0:consistency1__condition1:consistency1")) + theme_classic()

# Better pp_check (with m0 instead of m0_prior now)
y_pred <- posterior_linpred(m2_)
dens(inv_logit(y_pred))
# See centring around 25% - that is a propensity for answering correctly (25% probability of answering incorrect). Probability scale. And the range of values all look possible.
# Not comparing with actual data. Can in principle alo be compared with the actual data (he will not do that now though)

## Posterior update check (Has the posterior learned from the prior?)
plot(hypothesis(m2_,"condition0 > 0")) # has learned
plot(hypothesis(m2_,"condition1 > 0")) # has learned
plot(hypothesis(m2_,"condition0:consistency1 > 0")) # has not really gotten more confindent. One could go back to the prior and make sure that you do not too strongly influence the posterior
plot(hypothesis(m2_,"condition1:consistency1 > 0"))
# If the mean sat in the prior is pretty close to the data, then the posterior will not move that much in terms of the mean. But has it gotten more confidence (i.e. is it more narrow/has it learnt)?
# plot(hypothesis(m2_ SD???))
# He does not mention anything about actual hypothesis testing here...
```


Comparing error models for hypothesis 1
```{r}
# Adding criterion
m0 <- add_criterion(m0, criterion = c("bayes_R2", "loo"), reloo = T)
m1 <- add_criterion(m1, criterion = c("bayes_R2", "loo"), reloo = T)
m2 <- add_criterion(m2, criterion = c("bayes_R2", "loo"), reloo = T)
m0_ <- add_criterion(m0_, criterion = c("bayes_R2", "loo"), reloo = T)
m1_ <- add_criterion(m1_, criterion = c("bayes_R2", "loo"), reloo = T)
m2_ <- add_criterion(m2_, criterion = c("bayes_R2", "loo"), reloo = T)

# Comparing out of sample error
loo_compare(m0, m1, m2, m0_, m1_, m2_)

# Stacking weights
loo_model_weights(m0,m1, m2, m0_, m1_, m2_)

```




* Make some pretty plots
* Use the NDT in RT models
* How to approach H2?




Hypothesis 2
```{r}
# LOOK AT THIS LATER

prior_test <- c(
  prior(normal(0, 1.5), class = b))

mtest <- brm(
  RT_f2_,
  d,
  family = lognormal(),
  prior = prior_test,
  sample_prior = T
)

ranef(mtest, summary = T)
ranef(mtest, summary = F)
test <- data.frame((ranef(mtest, summary = F)))


# Which priors should be defined?
get_prior(RT_f3, d, family = lognormal())
```

